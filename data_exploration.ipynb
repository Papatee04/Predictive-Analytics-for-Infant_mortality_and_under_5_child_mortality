{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed\n",
      "Processing zip file: C:/dhs data/ZM_2005_HIVSPA_09242024_814_219681.zip\n",
      "Starting to load data from: C:/dhs data/ZM_2005_HIVSPA_09242024_814_219681.zip\n",
      "Zip file opened: C:/dhs data/ZM_2005_HIVSPA_09242024_814_219681.zip\n",
      "Found .dat file: ZMMS5AFLSR/ZMMS5AFLSR.DAT\n",
      "Found corresponding metadata file: ZMMS5AFLSR/ZMMS5AFLSR.SAS\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:574\u001b[0m, in \u001b[0;36mPythonParser._header_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 574\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffered_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:637\u001b[0m, in \u001b[0;36mPythonParser._buffered_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:738\u001b[0m, in \u001b[0;36mPythonParser._next_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 738\u001b[0m     orig_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_iter_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:805\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[1;34m(self, row_num)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 805\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1314\u001b[0m, in \u001b[0;36mFixedWidthReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1314\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;66;03m# Note: 'colspecs' is a sequence of half-open intervals.\u001b[39;00m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zip_file \u001b[38;5;129;01min\u001b[39;00m zip_files:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing zip file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m     \u001b[43mload_data_from_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScript execution completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 111\u001b[0m, in \u001b[0;36mload_data_from_zip\u001b[1;34m(zip_path, output_folder)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound corresponding metadata file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m z\u001b[38;5;241m.\u001b[39mopen(dat_file) \u001b[38;5;28;01mas\u001b[39;00m dat_f, z\u001b[38;5;241m.\u001b[39mopen(metadata_file) \u001b[38;5;28;01mas\u001b[39;00m meta_f:\n\u001b[1;32m--> 111\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_dat_with_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextIOWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdat_f\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextIOWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_f\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(dat_file))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    113\u001b[0m     data_dict[key] \u001b[38;5;241m=\u001b[39m df\n",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m, in \u001b[0;36mload_dat_with_metadata\u001b[1;34m(dat_file, metadata_file)\u001b[0m\n\u001b[0;32m     67\u001b[0m     names\u001b[38;5;241m.\u001b[39mappend(var[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Load the .dat file as fixed-width format\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_fwf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdat_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolspecs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolspecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1565\u001b[0m, in \u001b[0;36mread_fwf\u001b[1;34m(filepath_or_buffer, colspecs, widths, infer_nrows, dtype_backend, iterator, chunksize, **kwds)\u001b[0m\n\u001b[0;32m   1563\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m   1564\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dtype_backend\n\u001b[1;32m-> 1565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1329\u001b[0m, in \u001b[0;36mFixedWidthFieldParser.__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolspecs \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolspecs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer_nrows \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer_nrows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1329\u001b[0m \u001b[43mPythonParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:133\u001b[0m, in \u001b[0;36mPythonParser.__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    128\u001b[0m columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]]\n\u001b[0;32m    129\u001b[0m (\n\u001b[0;32m    130\u001b[0m     columns,\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_original_columns,\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols,\n\u001b[1;32m--> 133\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Now self.columns has the set of columns that we will process.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# The original set is stored in self.original_columns.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'index_names'\u001b[39;00m\n\u001b[0;32m    138\u001b[0m (\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    146\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:546\u001b[0m, in \u001b[0;36mPythonParser._infer_columns\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    542\u001b[0m         columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_usecols(\n\u001b[0;32m    543\u001b[0m             columns, columns[\u001b[38;5;241m0\u001b[39m], num_original_columns\n\u001b[0;32m    544\u001b[0m         )\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 546\u001b[0m     ncols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_header_line\u001b[49m)\n\u001b[0;32m    547\u001b[0m     num_original_columns \u001b[38;5;241m=\u001b[39m ncols\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m names:\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tlche\\OneDrive\\Documents\\GitHub\\Predictive-Analytics-for-Tuberculosis-TB-Incidence-and-Treatment-Adherence\\tbvenv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:577\u001b[0m, in \u001b[0;36mPythonParser._header_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames:\n\u001b[1;32m--> 577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EmptyDataError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo columns to parse from file\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    579\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames[:]\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m line\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "\n",
    "print(\"Imports completed\")\n",
    "\n",
    "def parse_metadata_file(metadata_file):\n",
    "    \"\"\"\n",
    "    Parse DHS metadata files (.dcf, .map, etc.) to extract variable information.\n",
    "    \"\"\"\n",
    "    variables = []\n",
    "    current_variable = None\n",
    "    in_variable_block = False\n",
    "\n",
    "    for line in metadata_file:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line.startswith(\"[\"):\n",
    "            # New section (could be a variable or something else)\n",
    "            in_variable_block = line == \"[Variable]\"\n",
    "            if in_variable_block:\n",
    "                current_variable = {}\n",
    "        elif in_variable_block:\n",
    "            if \"=\" in line:\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                key = key.strip().lower()\n",
    "                value = value.strip().strip('\"')\n",
    "                \n",
    "                if key == \"name\":\n",
    "                    current_variable[\"name\"] = value\n",
    "                elif key == \"label\":\n",
    "                    current_variable[\"label\"] = value\n",
    "                elif key == \"start\":\n",
    "                    current_variable[\"start\"] = int(value)\n",
    "                elif key == \"len\":\n",
    "                    current_variable[\"length\"] = int(value)\n",
    "            elif line == \"\":\n",
    "                # Empty line indicates end of variable block\n",
    "                if current_variable and \"name\" in current_variable:\n",
    "                    variables.append(current_variable)\n",
    "                current_variable = None\n",
    "                in_variable_block = False\n",
    "\n",
    "    # Add the last variable if the file ends without a blank line\n",
    "    if current_variable and \"name\" in current_variable:\n",
    "        variables.append(current_variable)\n",
    "\n",
    "    # Sort variables by their start position\n",
    "    variables.sort(key=lambda v: v.get(\"start\", 0))\n",
    "\n",
    "    return variables\n",
    "\n",
    "def load_dat_with_metadata(dat_file, metadata_file):\n",
    "    \"\"\"\n",
    "    Load .dat file with the corresponding metadata (.dcf, .map, etc.).\n",
    "    \"\"\"\n",
    "    variables = parse_metadata_file(metadata_file)\n",
    "\n",
    "    # Prepare column specifications for pd.read_fwf\n",
    "    colspecs = []\n",
    "    names = []\n",
    "    for var in variables:\n",
    "        start = var.get(\"start\", 0) - 1  # Adjust for 0-based index\n",
    "        length = var.get(\"length\", 1)\n",
    "        colspecs.append((start, start + length))\n",
    "        names.append(var[\"name\"])\n",
    "\n",
    "    # Load the .dat file as fixed-width format\n",
    "    df = pd.read_fwf(dat_file, colspecs=colspecs, names=names, encoding='latin1')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_csv(data_dict, output_folder):\n",
    "    \"\"\"\n",
    "    Save the data dictionary (datasets) into CSV files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for dataset_name, df in data_dict.items():\n",
    "        csv_file_path = os.path.join(output_folder, f\"{dataset_name}.csv\")\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"Saved {dataset_name} to {csv_file_path}\")\n",
    "\n",
    "def load_data_from_zip(zip_path, output_folder):\n",
    "    print(f\"Starting to load data from: {zip_path}\")\n",
    "    data_dict = {}\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        print(f\"Zip file opened: {zip_path}\")\n",
    "        \n",
    "        # Find all .dat files\n",
    "        dat_files = [f for f in z.namelist() if f.lower().endswith('.dat')]\n",
    "        \n",
    "        for dat_file in dat_files:\n",
    "            print(f\"Found .dat file: {dat_file}\")\n",
    "            dat_dir = os.path.dirname(dat_file)\n",
    "            dat_base = os.path.splitext(os.path.basename(dat_file))[0]\n",
    "            \n",
    "            # Search for metadata files\n",
    "            metadata_file = None\n",
    "            for f in z.namelist():\n",
    "                f_dir = os.path.dirname(f)\n",
    "                f_base = os.path.splitext(os.path.basename(f))[0]\n",
    "                if f_dir == dat_dir and f_base.startswith(dat_base) and f.lower().endswith(('.dct', '.dcf', '.map', '.sas', '.sps', '.do')):\n",
    "                    metadata_file = f\n",
    "                    break\n",
    "            \n",
    "            if metadata_file:\n",
    "                print(f\"Found corresponding metadata file: {metadata_file}\")\n",
    "                with z.open(dat_file) as dat_f, z.open(metadata_file) as meta_f:\n",
    "                    df = load_dat_with_metadata(io.TextIOWrapper(dat_f), io.TextIOWrapper(meta_f))\n",
    "                    key = os.path.splitext(os.path.basename(dat_file))[0]\n",
    "                    data_dict[key] = df\n",
    "                    print(f\"Added to data_dict with key: {key}\")\n",
    "            else:\n",
    "                print(f\"No corresponding metadata file found for: {dat_file}\")\n",
    "    \n",
    "    save_to_csv(data_dict, output_folder)\n",
    "    \n",
    "    print(f\"Finished loading data from: {zip_path}. Total datasets loaded: {len(data_dict)}\")\n",
    "    return data_dict\n",
    "\n",
    "# Paths to your zip files\n",
    "zip_files = ['C:/dhs data/ZM_2005_HIVSPA_09242024_814_219681.zip', 'C:/dhs data/ZM_2018_DHS_09242024_89_219681.zip']\n",
    "output_folder = 'C:/dhs_data/processed_csv'\n",
    "\n",
    "# Load all datasets and save them as CSVs\n",
    "for zip_file in zip_files:\n",
    "    print(f\"Processing zip file: {zip_file}\")\n",
    "    load_data_from_zip(zip_file, output_folder)\n",
    "\n",
    "print(\"Script execution completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
